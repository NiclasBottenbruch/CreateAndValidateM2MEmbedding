{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#imports \r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from scipy.spatial import distance_matrix as get_dm\r\n",
    "from numba import njit, prange"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import JSON data\r\n",
    "\r\n",
    "with open(\"2021_05_25_apis/allEndpoints.json\",\"r\",encoding=\"utf-8\") as json_file:\r\n",
    "     apis = json.load(json_file)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Number of APIs: \",len(apis))\r\n",
    "#calculate number of endpoints\r\n",
    "endpoint_lens = [len(apis[i][\"endpoints\"]) for i in range(len(apis))]\r\n",
    "print(\"Total number of Endpoints: \",sum(endpoint_lens))\r\n",
    "print(\"AVG number of Endpoints: \", sum(endpoint_lens)/len(apis))\r\n",
    "apis[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datastructures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import operator\r\n",
    "\r\n",
    "\r\n",
    "def string_to_list(value):\r\n",
    "        s_list = list(value)\r\n",
    "        out_list = []\r\n",
    "        for i in range(len(s_list)):\r\n",
    "            if operator.contains('!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',s_list[i]):\r\n",
    "                out_list.append('.')\r\n",
    "            else:\r\n",
    "                if s_list[i].isupper():\r\n",
    "                    out_list.append('.')\r\n",
    "                    out_list.append(s_list[i].lower())\r\n",
    "                else:\r\n",
    "                    out_list.append(s_list[i])\r\n",
    "\r\n",
    "        out_string = \"\".join(out_list)\r\n",
    "        return [x for x in out_string.split('.') if x]\r\n",
    "    \r\n",
    "\r\n",
    "class Api:\r\n",
    "    def __init__(self,api_data):\r\n",
    "        self._raw_data = api_data\r\n",
    "        self.key = api_data[\"key\"]\r\n",
    "        self.name = api_data[\"name\"]\r\n",
    "        self.version_key = api_data[\"versionKey\"]\r\n",
    "        self.version_name = api_data[\"versionName\"]\r\n",
    "        self.endpoints = [Endpoint(api_data[\"endpoints\"][i],i) for i in range(len(api_data[\"endpoints\"]))]\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def has_endpoints(self):\r\n",
    "        return len(self.endpoints) != 0\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"name\"] = self.name\r\n",
    "        json_dict[\"key\"] = self.key\r\n",
    "        json_dict[\"version_name\"] = self.version_name\r\n",
    "        json_dict[\"version_key\"] = self.version_key\r\n",
    "        json_dict[\"endpoints_size\"] = len(self.endpoints)\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "        \r\n",
    "    \r\n",
    "    \r\n",
    "        \r\n",
    "class Endpoint:\r\n",
    "    def __init__(self,endpoint_data,endpoint_num):\r\n",
    "        self._raw_data = endpoint_data\r\n",
    "        self.path = endpoint_data[\"path\"]\r\n",
    "        self.method = endpoint_data[\"method\"]\r\n",
    "        self.request_parameters = [Parameter(endpoint_data[\"requestParameters\"][i]) for i in range(len(endpoint_data[\"requestParameters\"]))]\r\n",
    "        self.response_parameters = [Parameter(endpoint_data[\"responseParameters\"][i]) for i in range(len(endpoint_data[\"responseParameters\"]))]\r\n",
    "        self.path_list = self.path_to_list()\r\n",
    "        self.num = endpoint_num\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def has_parameters(self):\r\n",
    "        return (len(self.request_parameters) != 0 or len(self.response_parameters) != 0)\r\n",
    "    \r\n",
    "    def path_to_list(self):\r\n",
    "        return string_to_list(self.path)\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"method\"] = self.method\r\n",
    "        json_dict[\"path\"] = self.path\r\n",
    "        json_dict[\"request_parameters_size\"] = len(self.request_parameters)\r\n",
    "        json_dict[\"response_parameters_size\"] = len(self.response_parameters)\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "class Parameter:\r\n",
    "    def __init__(self,parameter_data):\r\n",
    "        self._raw_data = parameter_data\r\n",
    "        self.xpath = parameter_data[\"xpath\"]\r\n",
    "        self.name = parameter_data[\"name\"]\r\n",
    "        \r\n",
    "        #if xpath is empty, overwrite it with name\r\n",
    "        if not self.xpath:\r\n",
    "            self.xpath = self.name\r\n",
    "            \r\n",
    "        self.xpath_list = self.xpath_to_list()\r\n",
    "        self.name_list = self.name_to_list()\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def name_to_list(self):\r\n",
    "        return string_to_list(self.name)\r\n",
    "        \r\n",
    "    def xpath_to_list(self):\r\n",
    "        return string_to_list(self.xpath)\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"name\"] = self.name\r\n",
    "        json_dict[\"xpath\"] = self.xpath\r\n",
    "        json_dict[\"name_list\"] = self.name_list\r\n",
    "        json_dict[\"xpath_list\"] = self.xpath_list\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "    \r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "apis_list = [Api(apis[i]) for i in range(len(apis))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test: print data for one api\r\n",
    "api = 2\r\n",
    "\r\n",
    "#Some tests:\r\n",
    "print(\"API:\")\r\n",
    "print(apis_list[api])\r\n",
    "print()\r\n",
    "print(\"Endpoints:\")\r\n",
    "\r\n",
    "for endpoint in apis_list[api].endpoints:\r\n",
    "    print(\"Endpoint:\")\r\n",
    "    print(endpoint)\r\n",
    "    print()\r\n",
    "    print(\"request parameters:\")\r\n",
    "    for rp in endpoint.request_parameters:\r\n",
    "        print(rp)\r\n",
    "    print()\r\n",
    "    print(\"response parameters:\")\r\n",
    "    for rp in endpoint.response_parameters:\r\n",
    "        print(rp)\r\n",
    "    print()\r\n",
    "    print()\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# functions to load embeddings from file\r\n",
    "\r\n",
    "def load_embedding_from_json(file_path:str):    \r\n",
    "    with open(file_path,\"r\",encoding=\"utf-8\") as json_file:\r\n",
    "         word_embedding = json.load(json_file)       \r\n",
    "    # convert vectors from list to np array\r\n",
    "    for key, vector in word_embedding.items():\r\n",
    "        word_embedding[key] = np.array(vector)\r\n",
    "    return word_embedding\r\n",
    "\r\n",
    "def load_glove_embedding_from_file(file_path):\r\n",
    "    df = pd.read_csv(file_path, sep=\" \", quoting=3, header=None, index_col=0)\r\n",
    "    glove = {key: val.values for key, val in df.T.items()}\r\n",
    "    return glove"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load open api embedding \r\n",
    "api_embedding = load_embedding_from_json(\"saved_embeddings/open_api_embedding_5d_314_words.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load glove embedding\r\n",
    "glove_embedding = load_glove_embedding_from_file(\"saved_embeddings/glove.6B.50d.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reduce Glove Embedding to same Vocabulary as OpenAPI Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_words_and_vectors_from_embedding(embedding: dict):\r\n",
    "    words = []\r\n",
    "    vectors = []\r\n",
    "    \r\n",
    "    for word, vector in embedding.items():\r\n",
    "        words.append(word)\r\n",
    "        vectors.append(vector)\r\n",
    "    \r\n",
    "    return words, np.array(vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "api_embedding_words, api_embedding_vectors = extract_words_and_vectors_from_embedding(api_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reduce_embedding_to_words(embedding: dict, words: list):\r\n",
    "    reduced_embedding = {}\r\n",
    "    for w in words:\r\n",
    "        try:\r\n",
    "            vector = embedding[w]\r\n",
    "            reduced_embedding[w] = vector\r\n",
    "        except:\r\n",
    "            pass\r\n",
    "    return reduced_embedding    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reduce glove embedding to the same words as api embedding\r\n",
    "glove_embedding = reduce_embedding_to_words(glove_embedding, api_embedding_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Size own embedding: \"+str(len(api_embedding)))\r\n",
    "print(\"Size glove embedding: \"+str(len(glove_embedding)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datastructure for Requests "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Request:\r\n",
    "    def __init__(self, endpoint_name:str, method:str, parameters:list):\r\n",
    "        self.endpoint = endpoint_name\r\n",
    "        self.method = method\r\n",
    "        self.parameters = parameters\r\n",
    "        \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"endpoint\"] = self.endpoint\r\n",
    "        json_dict[\"method\"] = self.method\r\n",
    "        json_dict[\"parameters\"] = self.parameters\r\n",
    "        return json.dumps(json_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = Request(\"mein endpoint\", \"post\", [\"das\", \"hier\", \"ist\", \"schoen\"])\r\n",
    "\r\n",
    "print(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Request Object for requests that are completely embedded "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "def create_request_obj_if_embedded(embedding:dict, endpoint, min_words:int, max_words:int):\r\n",
    "    is_suitable = True\r\n",
    "    parameter_words = []\r\n",
    "    for request_parameter in endpoint.request_parameters:\r\n",
    "        for word in request_parameter.xpath_list:\r\n",
    "            if word in embedding:\r\n",
    "                parameter_words.append(word)\r\n",
    "            else:\r\n",
    "                is_suitable = False\r\n",
    "                break\r\n",
    "        if not is_suitable:\r\n",
    "            break\r\n",
    "    \r\n",
    "    parameter_words = list(set(parameter_words))\r\n",
    "    \r\n",
    "    if len(parameter_words) < min_words or len(parameter_words) > max_words:\r\n",
    "        is_suitable = False        \r\n",
    "    \r\n",
    "    if is_suitable:\r\n",
    "        request_obj = Request(endpoint.path[1:], endpoint.method, parameter_words)\r\n",
    "    else:\r\n",
    "        request_obj = None\r\n",
    "        \r\n",
    "    return is_suitable, request_obj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "requests = []\r\n",
    "\r\n",
    "for api in apis_list:\r\n",
    "    for endpoint in api.endpoints:\r\n",
    "        is_embedded, request_obj = create_request_obj_if_embedded(glove_embedding, endpoint, min_words=4, max_words=4)\r\n",
    "        \r\n",
    "        if is_embedded:\r\n",
    "            requests.append(request_obj)\r\n",
    "        \r\n",
    "len(requests)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(requests[5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import cmp_to_key\r\n",
    "def compare(item1, item2):\r\n",
    "    if len(item1.parameters) < len(item2.parameters):\r\n",
    "        return 1\r\n",
    "    elif len(item1.parameters) > len(item2.parameters):\r\n",
    "        return -1\r\n",
    "    else:\r\n",
    "        return 0\r\n",
    "    \r\n",
    "requests.sort(key=cmp_to_key(compare))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(requests[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# nearest words in vector space\r\n",
    "\r\n",
    "from scipy.spatial import distance\r\n",
    "from functools import cmp_to_key\r\n",
    "\r\n",
    "def get_nearest_words(word_vector, embedding: dict):\r\n",
    "    def get_wordlist_distance_to_point(center_point, embedding:dict):\r\n",
    "        words = [] # list of elements (word, distance)\r\n",
    "        for word, vector in embedding.items():\r\n",
    "            words.append((distance.euclidean(center_point, vector), word))\r\n",
    "        return words\r\n",
    "            \r\n",
    "    def compare(item):\r\n",
    "            return item[0]\r\n",
    "    \r\n",
    "    words = get_wordlist_distance_to_point(word_vector, embedding)    \r\n",
    "    list.sort(words, key=compare)\r\n",
    "    \r\n",
    "    return words "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction with NN "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reduce Glove to 5 Dimensions with PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_embedding_words, glove_embedding_vectors = extract_words_and_vectors_from_embedding(glove_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "pca = PCA(n_components = 5)\r\n",
    "glove_embedding_vectors = pca.fit_transform(glove_embedding_vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_embedding_dict(embedding_words, embedding_vectors):\r\n",
    "    embedding = {}\r\n",
    "    \r\n",
    "    for i in range(len(embedding_words)):\r\n",
    "        embedding[embedding_words[i]] = embedding_vectors[i]\r\n",
    "        \r\n",
    "    return embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_embedding = create_embedding_dict(glove_embedding_words, glove_embedding_vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Requests into Training and Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "requests_train, requests_test = train_test_split(requests, test_size = 0.2, random_state = 0, shuffle=True)\r\n",
    "\r\n",
    "print(\"len all requests \"+str(len(requests)))\r\n",
    "print(\"len requests train \"+str(len(requests_train)))\r\n",
    "print(\"len requests test \"+str(len(requests_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Training/Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# all possible subsets of size k\r\n",
    "def get_all_possible_subsets_of_length_k_by_binomial_coefficient(number_to_choose_k:int, size_of_list_to_choose_from_n:int):\r\n",
    "    '''returns all combinations of (number_to_choose_k) indices in List with size (size_of_list_to_choose_from_n)'''\r\n",
    "    def get_next_combination(current_combination):\r\n",
    "        def move_later_indices_to_min_value(index_in_combination_just_set:int, current_combination):\r\n",
    "            value_just_set = current_combination[index_in_combination_just_set]\r\n",
    "            for i in range(1, len(current_combination) - index_in_combination_just_set):\r\n",
    "                current_combination[index_in_combination_just_set+i] = value_just_set+i\r\n",
    "            return current_combination\r\n",
    "        \r\n",
    "        for pos in range(len(current_combination)-1, -1, -1): # iterate backwards through list\r\n",
    "            pos_value = current_combination[pos]\r\n",
    "            if pos_value+1 < size_of_list_to_choose_from_n and pos_value+1 not in current_combination:\r\n",
    "                current_combination[pos] += 1\r\n",
    "                current_combination = move_later_indices_to_min_value(pos, current_combination)\r\n",
    "                break\r\n",
    "        return current_combination\r\n",
    "\r\n",
    "    sets = []\r\n",
    "    combination = [i for i in range(number_to_choose_k)]\r\n",
    "    last_combination = [i for i in range(size_of_list_to_choose_from_n-number_to_choose_k, size_of_list_to_choose_from_n)]\r\n",
    "\r\n",
    "    sets.append(combination.copy())\r\n",
    "    while combination != last_combination:\r\n",
    "        combination = get_next_combination(combination)\r\n",
    "        sets.append(combination.copy())\r\n",
    "    return np.array(sets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_all_possible_subsets_of_length_k_by_binomial_coefficient(3, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get_data_and_wanted results from requests\r\n",
    "\r\n",
    "def get_data_for_training(requests, embedding:dict):\r\n",
    "    def get_input(words:list, embedding:dict):\r\n",
    "        input = []\r\n",
    "        for i in range(3):\r\n",
    "            try:\r\n",
    "                vector = embedding[words[i]]\r\n",
    "            except:\r\n",
    "                vector = np.zeros(5)\r\n",
    "            \r\n",
    "            input.append(vector)\r\n",
    "\r\n",
    "        # result are all permutations\r\n",
    "        result = np.array([np.concatenate((input[0],input[1],input[2])),\r\n",
    "                           np.concatenate((input[0],input[2],input[1])),\r\n",
    "                           np.concatenate((input[1],input[0],input[2])),\r\n",
    "                           np.concatenate((input[1],input[2],input[1])),\r\n",
    "                           np.concatenate((input[2],input[0],input[1])),\r\n",
    "                           np.concatenate((input[2],input[1],input[0]))])\r\n",
    "        return result\r\n",
    "\r\n",
    "    input_data = []\r\n",
    "    labels = []\r\n",
    "    size_output = len(embedding)\r\n",
    "\r\n",
    "    for req in requests:\r\n",
    "        param = req.parameters\r\n",
    "\r\n",
    "        index_variants_known_words = get_all_possible_subsets_of_length_k_by_binomial_coefficient(3, len(param))\r\n",
    "\r\n",
    "        for indexes in index_variants_known_words:\r\n",
    "            w0 = param[indexes[0]]\r\n",
    "            w1 = param[indexes[1]]\r\n",
    "            w2 = param[indexes[2]]\r\n",
    "            words = [w0, w1, w2]\r\n",
    "\r\n",
    "            new_inputs = get_input(words, embedding)\r\n",
    "\r\n",
    "            # len(new_inputs) should be 6 (permutation of 3 input embeddings)\r\n",
    "\r\n",
    "            words_to_predict = param.copy()\r\n",
    "            words_to_predict.remove(w0)\r\n",
    "            words_to_predict.remove(w1)\r\n",
    "            words_to_predict.remove(w2)\r\n",
    "\r\n",
    "            wanted_output = embedding[words_to_predict[0]]\r\n",
    "\r\n",
    "            for ni in new_inputs:\r\n",
    "                input_data.append(ni)\r\n",
    "                labels.append(wanted_output)\r\n",
    "\r\n",
    "    return np.array(input_data), np.array(labels)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training / test data\r\n",
    "input_train_api, output_train_api = get_data_for_training(requests_train, api_embedding)\r\n",
    "input_test_api, output_test_api = get_data_for_training(requests_test, api_embedding)\r\n",
    "\r\n",
    "print(input_train_api.shape)\r\n",
    "print(output_train_api.shape)\r\n",
    "print()\r\n",
    "print(input_test_api.shape)\r\n",
    "print(output_test_api.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training / test data glove embedding\r\n",
    "input_train_glove, output_train_glove = get_data_for_training(requests_train, glove_embedding)\r\n",
    "input_test_glove, output_test_glove = get_data_for_training(requests_test, glove_embedding)\r\n",
    "\r\n",
    "print(input_train_glove.shape)\r\n",
    "print(output_train_glove.shape)\r\n",
    "print()\r\n",
    "print(input_test_glove.shape)\r\n",
    "print(output_test_glove.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# shuffle api data\r\n",
    "random_seed = 42  # guarantees that input and labels are shuffeled in the same way\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(input_train_api)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(output_train_api)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(input_test_api)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(output_test_api)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# shuffle glove data\r\n",
    "random_seed = 42  # guarantees that input and labels are shuffeled in the same way\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(input_train_glove)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(output_train_glove)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(input_test_glove)\r\n",
    "\r\n",
    "np.random.seed(random_seed)\r\n",
    "np.random.shuffle(output_test_glove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import tensorflow.keras.backend as kb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model API Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_api_embedding = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "model_with_api_embedding.add(tf.keras.layers.Dense(units=15, activation='linear')) # input layer\r\n",
    "model_with_api_embedding.add(tf.keras.layers.Dense(units=32, activation='relu'))   # hidden layer\r\n",
    "model_with_api_embedding.add(tf.keras.layers.Dense(units=5, activation='linear')) # output layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_api_embedding.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Glove Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "model_with_glove_embedding.add(tf.keras.layers.Dense(units=15, activation='linear')) # input layer\r\n",
    "model_with_glove_embedding.add(tf.keras.layers.Dense(units=32, activation='relu'))   # hidden layer\r\n",
    "model_with_glove_embedding.add(tf.keras.layers.Dense(units=5, activation='linear')) # output layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test / Train Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_vector_to_word_dict(embedding:dict):\r\n",
    "    res = {}\r\n",
    "    for word, vec in embedding.items():\r\n",
    "        res[vec.tobytes()] = word\r\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create vector to word embeddings\r\n",
    "api_vector_to_word_dict = get_vector_to_word_dict(api_embedding)\r\n",
    "glove_vector_to_word_dict = get_vector_to_word_dict(glove_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_delay_of_prediction(res_predicted, res_actual, input_data, embedding:dict, vector_to_word_dict:dict)->int:\r\n",
    "    nearest_words = get_nearest_words(res_predicted, embedding)\r\n",
    "\r\n",
    "    #remove input words from result\r\n",
    "    input_word_1 = vector_to_word_dict[input_data[0:5].tobytes()]\r\n",
    "    input_word_2 = vector_to_word_dict[input_data[5:10].tobytes()]\r\n",
    "    input_word_3 = vector_to_word_dict[input_data[10:15].tobytes()]\r\n",
    "\r\n",
    "    words_to_remove = []\r\n",
    "\r\n",
    "    for i, el in enumerate(nearest_words):\r\n",
    "        if el[1] == input_word_1 or el[1] == input_word_2 or el[1] == input_word_3:\r\n",
    "            words_to_remove.append(el[1])\r\n",
    "    \r\n",
    "    for w in words_to_remove:\r\n",
    "        words_to_remove.remove(w)\r\n",
    "\r\n",
    "    word_to_predict =  vector_to_word_dict[res_actual.tobytes()]\r\n",
    "\r\n",
    "    # get delay\r\n",
    "    for i, el in enumerate(nearest_words):\r\n",
    "        if el[1] == word_to_predict:\r\n",
    "            return i\r\n",
    "            break\r\n",
    "\r\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_avg_prediction_delay(predictions_outputs, wanted_outputs, input_data, embedding, vector_to_word_dict):\r\n",
    "    delays = []\r\n",
    "\r\n",
    "    for i in range(len(predictions_outputs)):\r\n",
    "        delays.append(get_delay_of_prediction(predictions_outputs[i], wanted_outputs[i], input_data[i], embedding, vector_to_word_dict))\r\n",
    "\r\n",
    "    delays = np.array(delays)\r\n",
    "    avg_delay = np.sum(delays)/len(delays)\r\n",
    "\r\n",
    "    if None in delays:\r\n",
    "        print(\"a delay is None\")\r\n",
    "\r\n",
    "    return avg_delay"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result without Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result without Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 1 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 1)\r\n",
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 2 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 1)\r\n",
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 3 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 1)\r\n",
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 4 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 1)\r\n",
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 5 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result after 10 Epochs Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 10 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_with_glove_embedding.fit(input_train_glove, output_train_glove, batch_size = 4, epochs = 10)\r\n",
    "model_with_api_embedding.fit(input_train_api, output_train_api, batch_size = 4, epochs = 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_test_api = model_with_api_embedding.predict(input_test_api)\r\n",
    "prediction_train_api = model_with_api_embedding.predict(input_train_api)\r\n",
    "\r\n",
    "prediction_test_glove = model_with_glove_embedding.predict(input_test_glove)\r\n",
    "prediction_train_glove = model_with_glove_embedding.predict(input_train_glove)\r\n",
    "\r\n",
    "print(\"Result after 20 Epochs of Training\")\r\n",
    "avg_delay_api_test = get_avg_prediction_delay(prediction_test_api, output_test_api, input_test_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_test: \"+str(avg_delay_api_test))\r\n",
    "avg_delay_glove_test = get_avg_prediction_delay(prediction_test_glove, output_test_glove, input_test_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_test: \"+str(avg_delay_glove_test))\r\n",
    "avg_delay_api_train = get_avg_prediction_delay(prediction_train_api, output_train_api, input_train_api, api_embedding, api_vector_to_word_dict)\r\n",
    "print(\"avg_delay_api_train: \"+str(avg_delay_api_train))\r\n",
    "avg_delay_glove_train = get_avg_prediction_delay(prediction_train_glove, output_train_glove, input_train_glove, glove_embedding, glove_vector_to_word_dict)\r\n",
    "print(\"avg_delay_glove_train: \"+str(avg_delay_glove_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect redundancy and inconsistency in training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "number_data_input = len(input_train_api) + len(input_test_api)\r\n",
    "\r\n",
    "input_data_list = np.concatenate((input_train_api,input_test_api)).tolist()\r\n",
    "\r\n",
    "input_data_list_of_tuples = []\r\n",
    "for el in input_data_list:\r\n",
    "    input_data_list_of_tuples.append(tuple(el))\r\n",
    "\r\n",
    "input_data_unique = list(set(input_data_list_of_tuples))\r\n",
    "\r\n",
    "# convert tuples back to list\r\n",
    "for i, el in enumerate(input_data_unique):\r\n",
    "    input_data_unique[i] = [k for k in el]\r\n",
    "\r\n",
    "\r\n",
    "number_data_input_unique = len(list(set(input_data_list_of_tuples)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(number_data_input)\r\n",
    "print(number_data_input_unique)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "interpreter": {
   "hash": "640a24517a88fa39ef71434eb926794dc1d9497c7aa18429429cd6b5a4543668"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}