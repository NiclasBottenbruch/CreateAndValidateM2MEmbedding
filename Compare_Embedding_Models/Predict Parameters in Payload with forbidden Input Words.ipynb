{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#imports \r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from scipy.spatial import distance_matrix as get_dm\r\n",
    "from numba import njit, prange"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import JSON data\r\n",
    "\r\n",
    "with open(\"2021_05_25_apis/allEndpoints.json\",\"r\",encoding=\"utf-8\") as json_file:\r\n",
    "     apis = json.load(json_file)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Number of APIs: \",len(apis))\r\n",
    "#calculate number of endpoints\r\n",
    "endpoint_lens = [len(apis[i][\"endpoints\"]) for i in range(len(apis))]\r\n",
    "print(\"Total number of Endpoints: \",sum(endpoint_lens))\r\n",
    "print(\"AVG number of Endpoints: \", sum(endpoint_lens)/len(apis))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datastructures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import operator\r\n",
    "\r\n",
    "\r\n",
    "def string_to_list(value):\r\n",
    "        s_list = list(value)\r\n",
    "        out_list = []\r\n",
    "        for i in range(len(s_list)):\r\n",
    "            if operator.contains('!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',s_list[i]):\r\n",
    "                out_list.append('.')\r\n",
    "            else:\r\n",
    "                if s_list[i].isupper():\r\n",
    "                    out_list.append('.')\r\n",
    "                    out_list.append(s_list[i].lower())\r\n",
    "                else:\r\n",
    "                    out_list.append(s_list[i])\r\n",
    "\r\n",
    "        out_string = \"\".join(out_list)\r\n",
    "        return [x for x in out_string.split('.') if x]\r\n",
    "    \r\n",
    "\r\n",
    "class Api:\r\n",
    "    def __init__(self,api_data):\r\n",
    "        self._raw_data = api_data\r\n",
    "        self.key = api_data[\"key\"]\r\n",
    "        self.name = api_data[\"name\"]\r\n",
    "        self.version_key = api_data[\"versionKey\"]\r\n",
    "        self.version_name = api_data[\"versionName\"]\r\n",
    "        self.endpoints = [Endpoint(api_data[\"endpoints\"][i],i) for i in range(len(api_data[\"endpoints\"]))]\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def has_endpoints(self):\r\n",
    "        return len(self.endpoints) != 0\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"name\"] = self.name\r\n",
    "        json_dict[\"key\"] = self.key\r\n",
    "        json_dict[\"version_name\"] = self.version_name\r\n",
    "        json_dict[\"version_key\"] = self.version_key\r\n",
    "        json_dict[\"endpoints_size\"] = len(self.endpoints)\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "        \r\n",
    "    \r\n",
    "    \r\n",
    "        \r\n",
    "class Endpoint:\r\n",
    "    def __init__(self,endpoint_data,endpoint_num):\r\n",
    "        self._raw_data = endpoint_data\r\n",
    "        self.path = endpoint_data[\"path\"]\r\n",
    "        self.method = endpoint_data[\"method\"]\r\n",
    "        self.request_parameters = [Parameter(endpoint_data[\"requestParameters\"][i]) for i in range(len(endpoint_data[\"requestParameters\"]))]\r\n",
    "        self.response_parameters = [Parameter(endpoint_data[\"responseParameters\"][i]) for i in range(len(endpoint_data[\"responseParameters\"]))]\r\n",
    "        self.path_list = self.path_to_list()\r\n",
    "        self.num = endpoint_num\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def has_parameters(self):\r\n",
    "        return (len(self.request_parameters) != 0 or len(self.response_parameters) != 0)\r\n",
    "    \r\n",
    "    def path_to_list(self):\r\n",
    "        return string_to_list(self.path)\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"method\"] = self.method\r\n",
    "        json_dict[\"path\"] = self.path\r\n",
    "        json_dict[\"request_parameters_size\"] = len(self.request_parameters)\r\n",
    "        json_dict[\"response_parameters_size\"] = len(self.response_parameters)\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "class Parameter:\r\n",
    "    def __init__(self,parameter_data):\r\n",
    "        self._raw_data = parameter_data\r\n",
    "        self.xpath = parameter_data[\"xpath\"]\r\n",
    "        self.name = parameter_data[\"name\"]\r\n",
    "        \r\n",
    "        #if xpath is empty, overwrite it with name\r\n",
    "        if not self.xpath:\r\n",
    "            self.xpath = self.name\r\n",
    "            \r\n",
    "        self.xpath_list = self.xpath_to_list()\r\n",
    "        self.name_list = self.name_to_list()\r\n",
    "    \r\n",
    "    def get_property(self,name):\r\n",
    "        return self._raw_data[name]\r\n",
    "    \r\n",
    "    def name_to_list(self):\r\n",
    "        return string_to_list(self.name)\r\n",
    "        \r\n",
    "    def xpath_to_list(self):\r\n",
    "        return string_to_list(self.xpath)\r\n",
    "    \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"name\"] = self.name\r\n",
    "        json_dict[\"xpath\"] = self.xpath\r\n",
    "        json_dict[\"name_list\"] = self.name_list\r\n",
    "        json_dict[\"xpath_list\"] = self.xpath_list\r\n",
    "        return json.dumps(json_dict)\r\n",
    "    \r\n",
    "    \r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "apis_list = [Api(apis[i]) for i in range(len(apis))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test: print data for one api\r\n",
    "api = 2\r\n",
    "\r\n",
    "#Some tests:\r\n",
    "print(\"API:\")\r\n",
    "print(apis_list[api])\r\n",
    "print()\r\n",
    "print(\"Endpoints:\")\r\n",
    "\r\n",
    "for endpoint in apis_list[api].endpoints:\r\n",
    "    print(\"Endpoint:\")\r\n",
    "    print(endpoint)\r\n",
    "    print()\r\n",
    "    print(\"request parameters:\")\r\n",
    "    for rp in endpoint.request_parameters:\r\n",
    "        print(rp)\r\n",
    "    print()\r\n",
    "    print(\"response parameters:\")\r\n",
    "    for rp in endpoint.response_parameters:\r\n",
    "        print(rp)\r\n",
    "    print()\r\n",
    "    print()\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# functions to load embeddings from file\r\n",
    "\r\n",
    "def load_embedding_from_json(file_path:str):    \r\n",
    "    with open(file_path,\"r\",encoding=\"utf-8\") as json_file:\r\n",
    "         word_embedding = json.load(json_file)       \r\n",
    "    # convert vectors from list to np array\r\n",
    "    for key, vector in word_embedding.items():\r\n",
    "        word_embedding[key] = np.array(vector)\r\n",
    "    return word_embedding\r\n",
    "\r\n",
    "def load_glove_embedding_from_file(file_path):\r\n",
    "    df = pd.read_csv(file_path, sep=\" \", quoting=3, header=None, index_col=0)\r\n",
    "    glove = {key: val.values for key, val in df.T.items()}\r\n",
    "    return glove"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load open api embedding \r\n",
    "api_embedding = load_embedding_from_json(\"saved_embeddings/open_api_embedding_5d_314_words.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load glove embedding\r\n",
    "glove_embedding = load_glove_embedding_from_file(\"saved_embeddings/glove.6B.50d.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reduce Glove Embedding to same Vocabulary as OpenAPI Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_words_and_vectors_from_embedding(embedding: dict):\r\n",
    "    words = []\r\n",
    "    vectors = []\r\n",
    "    \r\n",
    "    for word, vector in embedding.items():\r\n",
    "        words.append(word)\r\n",
    "        vectors.append(vector)\r\n",
    "    \r\n",
    "    return words, np.array(vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "api_embedding_words, api_embedding_vectors = extract_words_and_vectors_from_embedding(api_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reduce_embedding_to_words(embedding: dict, words: list):\r\n",
    "    reduced_embedding = {}\r\n",
    "    for w in words:\r\n",
    "        try:\r\n",
    "            vector = embedding[w]\r\n",
    "            reduced_embedding[w] = vector\r\n",
    "        except:\r\n",
    "            pass\r\n",
    "    return reduced_embedding    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reduce glove embedding to the same words as api embedding\r\n",
    "glove_embedding = reduce_embedding_to_words(glove_embedding, api_embedding_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Size own embedding: \"+str(len(api_embedding)))\r\n",
    "print(\"Size glove embedding: \"+str(len(glove_embedding)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datastructure for Requests "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Request:\r\n",
    "    def __init__(self, endpoint_name:str, method:str, parameters:list):\r\n",
    "        self.endpoint = endpoint_name\r\n",
    "        self.method = method\r\n",
    "        self.parameters = parameters\r\n",
    "        \r\n",
    "    def __str__(self):\r\n",
    "        json_dict = {}\r\n",
    "        json_dict[\"endpoint\"] = self.endpoint\r\n",
    "        json_dict[\"method\"] = self.method\r\n",
    "        json_dict[\"parameters\"] = self.parameters\r\n",
    "        return json.dumps(json_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Request Object for requests that are completely embedded "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "def create_request_obj_if_embedded(embedding:dict, endpoint, min_words:int):\r\n",
    "    is_suitable = True\r\n",
    "    parameter_words = []\r\n",
    "    for request_parameter in endpoint.request_parameters:\r\n",
    "        for word in request_parameter.xpath_list:\r\n",
    "            if word in embedding:\r\n",
    "                parameter_words.append(word)\r\n",
    "            else:\r\n",
    "                is_suitable = False\r\n",
    "                break\r\n",
    "        if not is_suitable:\r\n",
    "            break\r\n",
    "    \r\n",
    "    parameter_words = list(set(parameter_words))\r\n",
    "    \r\n",
    "    if len(parameter_words) < min_words:\r\n",
    "        is_suitable = False        \r\n",
    "    \r\n",
    "    if is_suitable:\r\n",
    "        request_obj = Request(endpoint.path[1:], endpoint.method, parameter_words)\r\n",
    "    else:\r\n",
    "        request_obj = None\r\n",
    "        \r\n",
    "    return is_suitable, request_obj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "requests = []\r\n",
    "\r\n",
    "for api in apis_list:\r\n",
    "    for endpoint in api.endpoints:\r\n",
    "        is_embedded, request_obj = create_request_obj_if_embedded(glove_embedding, endpoint, min_words=4)\r\n",
    "        \r\n",
    "        if is_embedded:\r\n",
    "            requests.append(request_obj)\r\n",
    "        \r\n",
    "len(requests)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(requests[5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import cmp_to_key\r\n",
    "def compare(item1, item2):\r\n",
    "    if len(item1.parameters) < len(item2.parameters):\r\n",
    "        return 1\r\n",
    "    elif len(item1.parameters) > len(item2.parameters):\r\n",
    "        return -1\r\n",
    "    else:\r\n",
    "        return 0\r\n",
    "    \r\n",
    "requests.sort(key=cmp_to_key(compare))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(requests[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict next words by Distance in Vector Space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.spatial import distance\r\n",
    "from functools import cmp_to_key\r\n",
    "\r\n",
    "def get_next_words(known_words:list, embedding: dict):\r\n",
    "    def get_wordlist_distance_to_point(center_point, embedding:dict, known_words:list):\r\n",
    "        words = [] # list of elements (word, distance)\r\n",
    "        for word, vector in embedding.items():\r\n",
    "            if word not in known_words:\r\n",
    "                words.append((distance.euclidean(center_point, vector), word))\r\n",
    "        return words\r\n",
    "            \r\n",
    "    def compare(item):\r\n",
    "            return item[0]\r\n",
    "    \r\n",
    "    center_point = embedding[known_words[0]]\r\n",
    "    for i in range(1, len(known_words)):\r\n",
    "        center_point = np.add(center_point, embedding[known_words[i]])\r\n",
    "    center_point = center_point/len(known_words)\r\n",
    "    \r\n",
    "    words = get_wordlist_distance_to_point(center_point, embedding, known_words)    \r\n",
    "    list.sort(words, key=compare)\r\n",
    "    \r\n",
    "    return words "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_prediction_result(request, number_known_words:int, embedding:dict):\r\n",
    "    known_words = request.parameters[:number_known_words]\r\n",
    "    words_to_predict = request.parameters[number_known_words:]\r\n",
    "    number_words_in_request = len(request.parameters)\r\n",
    "    \r\n",
    "    prediction_order = [i[1] for i in get_next_words(known_words, embedding)]\r\n",
    "    \r\n",
    "    prediction_delay = [max(0, prediction_order.index(w)-len(words_to_predict)+1) for w in words_to_predict]\r\n",
    "    \r\n",
    "    number_correct = 0\r\n",
    "    for i in prediction_delay:\r\n",
    "        if i==0:\r\n",
    "            number_correct += 1\r\n",
    "            \r\n",
    "    \r\n",
    "    return number_correct/len(prediction_delay), sum(prediction_delay)/len(prediction_delay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "\r\n",
    "req = requests[1]\r\n",
    "num_param_in_reqest = len(req.parameters)\r\n",
    "known = random.randint(1, num_param_in_reqest-1)\r\n",
    "\r\n",
    "portion_correct_api, avg_delay_api_embedding = get_prediction_result(req, known, api_embedding)\r\n",
    "portion_correct_glove, avg_delay_glove_embedding = get_prediction_result(req, known, glove_embedding)\r\n",
    "\r\n",
    "print(\"portion correct own embedding: \"+str(portion_correct_api))\r\n",
    "print(\"avg delay own embedding: \"+str(avg_delay_api_embedding))\r\n",
    "print()\r\n",
    "print(\"portion correct glove embedding: \"+str(portion_correct_glove))\r\n",
    "print(\"avg delay glove embedding: \"+str(avg_delay_glove_embedding))\r\n",
    "print()\r\n",
    "print(\"known: \"+str(known))\r\n",
    "print(\"num_param_in_reqest: \"+str(num_param_in_reqest))\r\n",
    "print()\r\n",
    "print(\"known parameters: \"+str(req.parameters[:known]))\r\n",
    "print(\"parameters to predict : \"+str(req.parameters[known:]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\r\n",
    "\r\n",
    "error_own_embedding = []\r\n",
    "error_glove_embedding = []\r\n",
    "\r\n",
    "for request in requests:\r\n",
    "    known = math.floor(len(request.parameters)/2)\r\n",
    "    portion_correct_api, avg_delay_api_embedding = get_prediction_result(request, known, api_embedding)\r\n",
    "    portion_correct_glove, avg_delay_glove_embedding = get_prediction_result(request, known, glove_embedding)\r\n",
    "    \r\n",
    "    error_own_embedding.append(avg_delay_api_embedding)\r\n",
    "    error_glove_embedding.append(avg_delay_glove_embedding)\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"avg delay own embedding: \"+str(sum(error_own_embedding)/len(error_own_embedding)))\r\n",
    "print(\"avg delay glove embedding: \"+str(sum(error_glove_embedding)/len(error_glove_embedding)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction with NN "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reduce Glove to 5 Dimensions with PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_embedding_words, glove_embedding_vectors = extract_words_and_vectors_from_embedding(glove_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "pca = PCA(n_components = 5)\r\n",
    "glove_embedding_vectors = pca.fit_transform(glove_embedding_vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_embedding_dict(embedding_words, embedding_vectors):\r\n",
    "    embedding = {}\r\n",
    "    \r\n",
    "    for i in range(len(embedding_words)):\r\n",
    "        embedding[embedding_words[i]] = embedding_vectors[i]\r\n",
    "        \r\n",
    "    return embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_embedding = create_embedding_dict(glove_embedding_words, glove_embedding_vectors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardize Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Center the embedding arround the point of origin (sum of all vectors shall be zero)\n",
    "2. skale all Vectors linearily with a factor that the largest absolute value of any component of any vector is 1. <br>\n",
    "   --> every Component in every vector is in the range [-1;1]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def standardize_embedding(embedding: dict):\r\n",
    "    def extract_words_and_vectors_from_embedding(embedding: dict):\r\n",
    "        words = []\r\n",
    "        vectors = []\r\n",
    "        \r\n",
    "        for word, vector in embedding.items():\r\n",
    "            words.append(word)\r\n",
    "            vectors.append(vector)\r\n",
    "        \r\n",
    "        return words, np.array(vectors)\r\n",
    "\r\n",
    "    def relocate_vectors_to_center(vectors):\r\n",
    "        center = []\r\n",
    "        for i in range(len(vectors[0])):\r\n",
    "            center.append(np.sum(vectors[:,i]))\r\n",
    "        center = np.array(center)/len(vectors)\r\n",
    "        vectors -= center\r\n",
    "        return vectors \r\n",
    "    \r\n",
    "    words, vectors = extract_words_and_vectors_from_embedding(embedding)\r\n",
    "\r\n",
    "    vectors = relocate_vectors_to_center(vectors)\r\n",
    "\r\n",
    "    # scale vectors\r\n",
    "    scalar = 1/max(abs(np.amin(vectors)), np.amax(vectors))\r\n",
    "    print(\"scalar = \"+str(scalar))\r\n",
    "\r\n",
    "    for i in range(len(vectors)):\r\n",
    "        vectors[i] = vectors[i]*scalar\r\n",
    "    \r\n",
    "    # update embedding\r\n",
    "\r\n",
    "    for i, word in enumerate(words):\r\n",
    "        embedding[word] = vectors[i]\r\n",
    "    \r\n",
    "    return embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "api_embedding = standardize_embedding(api_embedding)\r\n",
    "glove_embedding = standardize_embedding(glove_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Forbidden Input Words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define 3 forbidden Input Words that occur at least once all in the same request:\n",
    "1. Pick random Request\n",
    "2. Pick 3 random words from that request"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_forbidden_words():\r\n",
    "    # take random request\r\n",
    "    param_to_choose_from = random.choice(requests).parameters\r\n",
    "\r\n",
    "    forbidden_words = []\r\n",
    "    while len(forbidden_words) < 3:\r\n",
    "        w = random.choice(param_to_choose_from)\r\n",
    "        if w not in forbidden_words:\r\n",
    "            forbidden_words.append(w)\r\n",
    "\r\n",
    "    return forbidden_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "forbidden_words = get_forbidden_words()\r\n",
    "forbidden_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Map Word to Index in One-Hot Vector"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_word_to_index_dict(wordlist:list):\r\n",
    "    res = {}\r\n",
    "    for i, word in enumerate(wordlist):\r\n",
    "        res[word] = i\r\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make word -> index dict\r\n",
    "index_of_word_glove = get_word_to_index_dict(glove_embedding_words)\r\n",
    "index_of_word_api = get_word_to_index_dict(api_embedding_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Training and Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# all possible subsets of size k\r\n",
    "def get_all_possible_subsets_of_length_k_by_binomial_coefficient(number_to_choose_k:int, size_of_list_to_choose_from_n:int):\r\n",
    "    '''returns all combinations of (number_to_choose_k) indices in List with size (size_of_list_to_choose_from_n)'''\r\n",
    "    def get_next_combination(current_combination):\r\n",
    "        def move_later_indices_to_min_value(index_in_combination_just_set:int, current_combination):\r\n",
    "            value_just_set = current_combination[index_in_combination_just_set]\r\n",
    "            for i in range(1, len(current_combination) - index_in_combination_just_set):\r\n",
    "                current_combination[index_in_combination_just_set+i] = value_just_set+i\r\n",
    "            return current_combination\r\n",
    "        \r\n",
    "        for pos in range(len(current_combination)-1, -1, -1): # iterate backwards through list\r\n",
    "            pos_value = current_combination[pos]\r\n",
    "            if pos_value+1 < size_of_list_to_choose_from_n and pos_value+1 not in current_combination:\r\n",
    "                current_combination[pos] += 1\r\n",
    "                current_combination = move_later_indices_to_min_value(pos, current_combination)\r\n",
    "                break\r\n",
    "        return current_combination\r\n",
    "\r\n",
    "    sets = []\r\n",
    "    combination = [i for i in range(number_to_choose_k)]\r\n",
    "    last_combination = [i for i in range(size_of_list_to_choose_from_n-number_to_choose_k, size_of_list_to_choose_from_n)]\r\n",
    "\r\n",
    "    sets.append(combination.copy())\r\n",
    "    while combination != last_combination:\r\n",
    "        combination = get_next_combination(combination)\r\n",
    "        sets.append(combination.copy())\r\n",
    "    return np.array(sets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_all_possible_subsets_of_length_k_by_binomial_coefficient(3, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get_data_and_wanted results from requests\r\n",
    "\r\n",
    "def get_data_for_training(requests, embedding:dict, index_of_word_dict: dict, forbidden_words:list):\r\n",
    "    def get_one_hot_output(words:list, index_of_word_dict: dict, size:int):\r\n",
    "        one_hot = np.zeros(size, dtype=np.float32)\r\n",
    "        for w in words:\r\n",
    "            one_hot[index_of_word_dict[w]] = 1\r\n",
    "        return one_hot\r\n",
    "\r\n",
    "    def get_input(words:list, embedding:dict):\r\n",
    "        input = []\r\n",
    "        for i in range(3):\r\n",
    "            try:\r\n",
    "                vector = embedding[words[i]]\r\n",
    "            except:\r\n",
    "                vector = np.zeros(5)\r\n",
    "            \r\n",
    "            input.append(vector)\r\n",
    "\r\n",
    "        # result are all permutations\r\n",
    "        result = np.array([np.concatenate((input[0],input[1],input[2])),\r\n",
    "                           np.concatenate((input[0],input[2],input[1])),\r\n",
    "                           np.concatenate((input[1],input[0],input[2])),\r\n",
    "                           np.concatenate((input[1],input[2],input[1])),\r\n",
    "                           np.concatenate((input[2],input[0],input[1])),\r\n",
    "                           np.concatenate((input[2],input[1],input[0]))])\r\n",
    "        return result\r\n",
    "\r\n",
    "\r\n",
    "    input_data_zero_forbidden_words = []\r\n",
    "    labels_zero_forbidden_words = []\r\n",
    "\r\n",
    "    input_data_one_forbidden_words = []\r\n",
    "    labels_one_forbidden_words = []\r\n",
    "\r\n",
    "    input_data_two_forbidden_words = []\r\n",
    "    labels_two_forbidden_words = []\r\n",
    "\r\n",
    "    input_data_three_forbidden_words = []\r\n",
    "    labels_three_forbidden_words = []\r\n",
    "\r\n",
    "\r\n",
    "    size_output = len(embedding)\r\n",
    "\r\n",
    "    for req in requests:\r\n",
    "        param = req.parameters\r\n",
    "\r\n",
    "        index_variants_known_words = get_all_possible_subsets_of_length_k_by_binomial_coefficient(3, len(param))\r\n",
    "\r\n",
    "        for indexes in index_variants_known_words:\r\n",
    "            w0 = param[indexes[0]]\r\n",
    "            w1 = param[indexes[1]]\r\n",
    "            w2 = param[indexes[2]]\r\n",
    "            words = [w0, w1, w2]\r\n",
    "\r\n",
    "            new_inputs = get_input(words, embedding)\r\n",
    "\r\n",
    "            # determine number forbidden words\r\n",
    "\r\n",
    "            num_forbidden_words_in_input = 0\r\n",
    "\r\n",
    "            for w in words:\r\n",
    "                if w in forbidden_words:\r\n",
    "                    num_forbidden_words_in_input += 1\r\n",
    "            \r\n",
    "            # 0 forbidden word in input\r\n",
    "            input_list = input_data_zero_forbidden_words\r\n",
    "            labels_list = labels_zero_forbidden_words\r\n",
    "\r\n",
    "            if num_forbidden_words_in_input == 1:\r\n",
    "                input_list = input_data_one_forbidden_words\r\n",
    "                labels_list = labels_one_forbidden_words\r\n",
    "\r\n",
    "            elif num_forbidden_words_in_input == 2:\r\n",
    "                input_list = input_data_two_forbidden_words\r\n",
    "                labels_list = labels_two_forbidden_words\r\n",
    "\r\n",
    "            elif num_forbidden_words_in_input == 3:\r\n",
    "                input_list = input_data_three_forbidden_words\r\n",
    "                labels_list = labels_three_forbidden_words\r\n",
    "            \r\n",
    "            \r\n",
    "            # len(new_inputs) should be 6 (permutation of 3 input embeddings)\r\n",
    "\r\n",
    "            words_to_predict = param.copy()\r\n",
    "            words_to_predict.remove(w0)\r\n",
    "            words_to_predict.remove(w1)\r\n",
    "            words_to_predict.remove(w2)\r\n",
    "\r\n",
    "            wanted_one_hot_output = get_one_hot_output(words_to_predict, index_of_word_dict, size_output)\r\n",
    "\r\n",
    "            for ni in new_inputs:\r\n",
    "                input_list.append(ni)\r\n",
    "                labels_list.append(wanted_one_hot_output)\r\n",
    "\r\n",
    "    return (np.array(input_data_zero_forbidden_words), np.array(labels_zero_forbidden_words),\r\n",
    "           np.array(input_data_one_forbidden_words), np.array(labels_one_forbidden_words),\r\n",
    "           np.array(input_data_two_forbidden_words), np.array(labels_two_forbidden_words),\r\n",
    "           np.array(input_data_three_forbidden_words), np.array(labels_three_forbidden_words))\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training / test data api embedding\r\n",
    "input_zero_forbidden_api, labels_zero_forbidden_api, input_one_forbidden_api, labels_one_forbidden_api, input_two_forbidden_api, labels_two_forbidden_api, input_three_forbidden_api, labels_three_forbidden_api = get_data_for_training(requests, api_embedding, index_of_word_api, forbidden_words)\r\n",
    "\r\n",
    "print(\"Data 0 forbidden input words\")\r\n",
    "print(input_zero_forbidden_api.shape)\r\n",
    "print(labels_zero_forbidden_api.shape)\r\n",
    "print()\r\n",
    "print(\"Data 1 forbidden input word\")\r\n",
    "print(input_one_forbidden_api.shape)\r\n",
    "print(labels_one_forbidden_api.shape)\r\n",
    "print()\r\n",
    "print(\"Data 2 forbidden input words\")\r\n",
    "print(input_two_forbidden_api.shape)\r\n",
    "print(labels_two_forbidden_api.shape)\r\n",
    "print()\r\n",
    "print(\"Data 3 forbidden input words\")\r\n",
    "print(input_three_forbidden_api.shape)\r\n",
    "print(labels_three_forbidden_api.shape)\r\n",
    "print()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training / test data api embedding\r\n",
    "input_zero_forbidden_glove, labels_zero_forbidden_glove, input_one_forbidden_glove, labels_one_forbidden_glove, input_two_forbidden_glove, labels_two_forbidden_glove, input_three_forbidden_glove, labels_three_forbidden_glove = get_data_for_training(requests, glove_embedding, index_of_word_glove, forbidden_words)\r\n",
    "\r\n",
    "print(\"Data 0 forbidden input words\")\r\n",
    "print(input_zero_forbidden_glove.shape)\r\n",
    "print(labels_zero_forbidden_glove.shape)\r\n",
    "print()\r\n",
    "print(\"Data 1 forbidden input word\")\r\n",
    "print(input_one_forbidden_glove.shape)\r\n",
    "print(labels_one_forbidden_glove.shape)\r\n",
    "print()\r\n",
    "print(\"Data 2 forbidden input words\")\r\n",
    "print(input_two_forbidden_glove.shape)\r\n",
    "print(labels_two_forbidden_glove.shape)\r\n",
    "print()\r\n",
    "print(\"Data 3 forbidden input words\")\r\n",
    "print(input_three_forbidden_glove.shape)\r\n",
    "print(labels_three_forbidden_glove.shape)\r\n",
    "print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_seed = 42  # guarantees that input and labels are shuffeled in the same way\r\n",
    "def shuffle_arrays(input_data, labels):\r\n",
    "    np.random.seed(random_seed)\r\n",
    "    np.random.shuffle(input_data)\r\n",
    "\r\n",
    "    np.random.seed(random_seed)\r\n",
    "    np.random.shuffle(labels)\r\n",
    "\r\n",
    "    return input_data, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# shuffle api data\r\n",
    "input_zero_forbidden_api, labels_zero_forbidden_api = shuffle_arrays(input_zero_forbidden_api, labels_zero_forbidden_api)\r\n",
    "input_one_forbidden_api, labels_one_forbidden_api = shuffle_arrays(input_one_forbidden_api, labels_one_forbidden_api)\r\n",
    "input_two_forbidden_api, labels_two_forbidden_api = shuffle_arrays(input_two_forbidden_api, labels_two_forbidden_api)\r\n",
    "input_three_forbidden_api, labels_three_forbidden_api = shuffle_arrays(input_three_forbidden_api, labels_three_forbidden_api)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# shuffle glove data\r\n",
    "input_zero_forbidden_glove, labels_zero_forbidden_glove = shuffle_arrays(input_zero_forbidden_glove, labels_zero_forbidden_glove)\r\n",
    "input_one_forbidden_glove, labels_one_forbidden_glove = shuffle_arrays(input_one_forbidden_glove, labels_one_forbidden_glove)\r\n",
    "input_two_forbidden_glove, labels_two_forbidden_glove = shuffle_arrays(input_two_forbidden_glove, labels_two_forbidden_glove)\r\n",
    "input_three_forbidden_glove, labels_three_forbidden_glove = shuffle_arrays(input_three_forbidden_glove, labels_three_forbidden_glove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import tensorflow.keras.backend as kb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model API Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_api_nn_model():\r\n",
    "    model_with_api_embedding = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "    model_with_api_embedding.add(tf.keras.layers.Dense(units=15, activation='linear')) # input layer\r\n",
    "    model_with_api_embedding.add(tf.keras.layers.Dense(units=128, activation='relu'))   # hidden layer\r\n",
    "    model_with_api_embedding.add(tf.keras.layers.Dense(units=len(labels_zero_forbidden_api[0]), activation='sigmoid')) # output layer\r\n",
    "\r\n",
    "    model_with_api_embedding.compile(optimizer = 'adam', loss = 'binary_crossentropy')\r\n",
    "    return model_with_api_embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Glove Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_glove_nn_model():\r\n",
    "    model_with_glove_embedding = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "    model_with_glove_embedding.add(tf.keras.layers.Dense(units=15, activation='linear')) # input layer\r\n",
    "    model_with_glove_embedding.add(tf.keras.layers.Dense(units=128, activation='relu'))   # hidden layer\r\n",
    "    model_with_glove_embedding.add(tf.keras.layers.Dense(units=len(labels_zero_forbidden_glove[0]), activation='sigmoid')) # output layer\r\n",
    "\r\n",
    "    model_with_glove_embedding.compile(optimizer = 'adam', loss = 'binary_crossentropy')\r\n",
    "    return model_with_glove_embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test / Train Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_nn_prediction_result(res_predicted, res_actual, log=False):\r\n",
    "    prediction = []\r\n",
    "\r\n",
    "    for i, value in enumerate(res_predicted):\r\n",
    "        prediction.append([i, value])\r\n",
    "        \r\n",
    "\r\n",
    "    def sortkey(el):\r\n",
    "        return el[1]\r\n",
    "\r\n",
    "    prediction.sort(key=sortkey, reverse=True)\r\n",
    "    if log:\r\n",
    "        print(\"prediction order:\")\r\n",
    "        print(prediction)\r\n",
    "\r\n",
    "    prediction_order = [i[0] for i in prediction]\r\n",
    "\r\n",
    "    prediction = None # free ram\r\n",
    "    \r\n",
    "    indices_words_to_predict = [i for i in range(len(res_actual)) if res_actual[i]==1]\r\n",
    "\r\n",
    "    if log:\r\n",
    "        print(\"indices to predict: \"+str(indices_words_to_predict))\r\n",
    "\r\n",
    "    prediction_delays = [max(0, prediction_order.index(w)-len(indices_words_to_predict)+1) for w in indices_words_to_predict]\r\n",
    "\r\n",
    "    if log:\r\n",
    "        print(\"Prediction delays: \"+str(prediction_delays))\r\n",
    "\r\n",
    "    number_correct = 0\r\n",
    "    for i in prediction_delays:\r\n",
    "        if i==0:\r\n",
    "            number_correct += 1\r\n",
    "\r\n",
    "    portion_correct = number_correct/len(prediction_delays)\r\n",
    "    avg_delay = sum(prediction_delays)/len(prediction_delays)\r\n",
    "    \r\n",
    "    return portion_correct, avg_delay"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statistics import median\r\n",
    "\r\n",
    "def get_avg_prediction_delay(predictions_outputs, wanted_outputs):\r\n",
    "    delays = []\r\n",
    "\r\n",
    "    for i in range(len(predictions_outputs)):\r\n",
    "        delays.append(get_nn_prediction_result(predictions_outputs[i], wanted_outputs[i])[1])\r\n",
    "\r\n",
    "    delays = np.array(delays)\r\n",
    "    avg_delay = np.sum(delays)/len(delays)\r\n",
    "\r\n",
    "    return avg_delay, median(delays)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perpetual Train and Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def perpetual_test_handle_forbidden_words():\r\n",
    "    while(True):\r\n",
    "        forbidden_words = get_forbidden_words()\r\n",
    "\r\n",
    "        # generate training and test data\r\n",
    "        input_zero_forbidden_api, labels_zero_forbidden_api, input_one_forbidden_api, labels_one_forbidden_api, input_two_forbidden_api, labels_two_forbidden_api, input_three_forbidden_api, labels_three_forbidden_api = get_data_for_training(requests, api_embedding, index_of_word_api, forbidden_words)\r\n",
    "\r\n",
    "        input_zero_forbidden_glove, labels_zero_forbidden_glove, input_one_forbidden_glove, labels_one_forbidden_glove, input_two_forbidden_glove, labels_two_forbidden_glove, input_three_forbidden_glove, labels_three_forbidden_glove = get_data_for_training(requests, glove_embedding, index_of_word_glove, forbidden_words)\r\n",
    "\r\n",
    "        # shuffle\r\n",
    "        input_zero_forbidden_api, labels_zero_forbidden_api = shuffle_arrays(input_zero_forbidden_api, labels_zero_forbidden_api)\r\n",
    "        input_one_forbidden_api, labels_one_forbidden_api = shuffle_arrays(input_one_forbidden_api, labels_one_forbidden_api)\r\n",
    "        input_two_forbidden_api, labels_two_forbidden_api = shuffle_arrays(input_two_forbidden_api, labels_two_forbidden_api)\r\n",
    "        input_three_forbidden_api, labels_three_forbidden_api = shuffle_arrays(input_three_forbidden_api, labels_three_forbidden_api)\r\n",
    "\r\n",
    "        input_zero_forbidden_glove, labels_zero_forbidden_glove = shuffle_arrays(input_zero_forbidden_glove, labels_zero_forbidden_glove)\r\n",
    "        input_one_forbidden_glove, labels_one_forbidden_glove = shuffle_arrays(input_one_forbidden_glove, labels_one_forbidden_glove)\r\n",
    "        input_two_forbidden_glove, labels_two_forbidden_glove = shuffle_arrays(input_two_forbidden_glove, labels_two_forbidden_glove)\r\n",
    "        input_three_forbidden_glove, labels_three_forbidden_glove = shuffle_arrays(input_three_forbidden_glove, labels_three_forbidden_glove)\r\n",
    "\r\n",
    "        # create model\r\n",
    "        model_with_api_embedding = create_api_nn_model()\r\n",
    "        model_with_glove_embedding = create_glove_nn_model()\r\n",
    "\r\n",
    "        print(\"-------------------------------------------------------------------------------------------------------------\")\r\n",
    "        print(\"-------------------------------------------------------------------------------------------------------------\")\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"forbidden input words: \"+str(forbidden_words))\r\n",
    "        print()\r\n",
    "        print(\"size data without forbidden input (training data) : \"+str(len(labels_zero_forbidden_api)))\r\n",
    "        print(\"size data with 1 forbidden word                   : \"+str(len(labels_one_forbidden_api)))\r\n",
    "        print(\"size data with 2 forbidden words                  : \"+str(len(labels_two_forbidden_api)))\r\n",
    "        print(\"size data with 3 forbidden words                  : \"+str(len(labels_three_forbidden_api)))\r\n",
    "        print()\r\n",
    "\r\n",
    "        prediction_zero_forbidden_api = model_with_api_embedding.predict(input_zero_forbidden_api)\r\n",
    "        prediction_zero_forbidden_glove = model_with_glove_embedding.predict(input_zero_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_one_forbidden_api = model_with_api_embedding.predict(input_one_forbidden_api)\r\n",
    "        prediction_one_forbidden_glove = model_with_glove_embedding.predict(input_one_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_two_forbidden_api = model_with_api_embedding.predict(input_two_forbidden_api)\r\n",
    "        prediction_two_forbidden_glove = model_with_glove_embedding.predict(input_two_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_three_forbidden_api = model_with_api_embedding.predict(input_three_forbidden_api)\r\n",
    "        prediction_three_forbidden_glove = model_with_glove_embedding.predict(input_three_forbidden_glove)\r\n",
    "\r\n",
    "        print(\"   ---   Result without Training   ---\")\r\n",
    "        print()\r\n",
    "        print(\"Result avg delay zero forbidden words (training data):\")\r\n",
    "        avg_delay_zero_forbidden_api, median_delay_zero_forbidden_api = get_avg_prediction_delay(prediction_zero_forbidden_api, labels_zero_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_zero_forbidden_api)+\" median = \"+str(median_delay_zero_forbidden_api))\r\n",
    "        avg_delay_zero_forbidden_glove, median_delay_zero_forbidden_glove = get_avg_prediction_delay(prediction_zero_forbidden_glove, labels_zero_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_zero_forbidden_glove)+\" median = \"+str(median_delay_zero_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay one forbidden word:\")\r\n",
    "        avg_delay_one_forbidden_api, median_delay_one_forbidden_api = get_avg_prediction_delay(prediction_one_forbidden_api, labels_one_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_one_forbidden_api)+\" median = \"+str(median_delay_one_forbidden_api))\r\n",
    "        avg_delay_one_forbidden_glove, median_delay_one_forbidden_glove = get_avg_prediction_delay(prediction_one_forbidden_glove, labels_one_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_one_forbidden_glove)+\" median = \"+str(median_delay_one_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay two forbidden words:\")\r\n",
    "        avg_delay_two_forbidden_api, median_delay_two_forbidden_api = get_avg_prediction_delay(prediction_two_forbidden_api, labels_two_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_two_forbidden_api)+\" median = \"+str(median_delay_two_forbidden_api))\r\n",
    "        avg_delay_two_forbidden_glove, median_delay_two_forbidden_glove = get_avg_prediction_delay(prediction_two_forbidden_glove, labels_two_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_two_forbidden_glove)+\" median = \"+str(median_delay_two_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay three forbidden words:\")\r\n",
    "        avg_delay_three_forbidden_api, median_delay_three_forbidden_api = get_avg_prediction_delay(prediction_three_forbidden_api, labels_three_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_three_forbidden_api)+\" median = \"+str(median_delay_three_forbidden_api))\r\n",
    "        avg_delay_three_forbidden_glove, median_delay_three_forbidden_glove = get_avg_prediction_delay(prediction_three_forbidden_glove, labels_three_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_three_forbidden_glove)+\" median = \"+str(median_delay_three_forbidden_glove))\r\n",
    "        print()\r\n",
    "        print()\r\n",
    "\r\n",
    "        # train\r\n",
    "        model_with_glove_embedding.fit(input_zero_forbidden_glove, labels_zero_forbidden_glove, batch_size = 8, epochs = 20)\r\n",
    "        model_with_api_embedding.fit(input_zero_forbidden_api, labels_zero_forbidden_api, batch_size = 8, epochs = 20)\r\n",
    "\r\n",
    "        # predict\r\n",
    "        prediction_zero_forbidden_api = model_with_api_embedding.predict(input_zero_forbidden_api)\r\n",
    "        prediction_zero_forbidden_glove = model_with_glove_embedding.predict(input_zero_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_one_forbidden_api = model_with_api_embedding.predict(input_one_forbidden_api)\r\n",
    "        prediction_one_forbidden_glove = model_with_glove_embedding.predict(input_one_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_two_forbidden_api = model_with_api_embedding.predict(input_two_forbidden_api)\r\n",
    "        prediction_two_forbidden_glove = model_with_glove_embedding.predict(input_two_forbidden_glove)\r\n",
    "\r\n",
    "        prediction_three_forbidden_api = model_with_api_embedding.predict(input_three_forbidden_api)\r\n",
    "        prediction_three_forbidden_glove = model_with_glove_embedding.predict(input_three_forbidden_glove)\r\n",
    "\r\n",
    "\r\n",
    "        print()\r\n",
    "        print(\"   ---   Result after 20 Epochs Training   ---\")\r\n",
    "        print()\r\n",
    "        print(\"Result avg delay zero forbidden words (training data):\")\r\n",
    "        avg_delay_zero_forbidden_api, median_delay_zero_forbidden_api = get_avg_prediction_delay(prediction_zero_forbidden_api, labels_zero_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_zero_forbidden_api)+\" median = \"+str(median_delay_zero_forbidden_api))\r\n",
    "        avg_delay_zero_forbidden_glove, median_delay_zero_forbidden_glove = get_avg_prediction_delay(prediction_zero_forbidden_glove, labels_zero_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_zero_forbidden_glove)+\" median = \"+str(median_delay_zero_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay one forbidden word:\")\r\n",
    "        avg_delay_one_forbidden_api, median_delay_one_forbidden_api = get_avg_prediction_delay(prediction_one_forbidden_api, labels_one_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_one_forbidden_api)+\" median = \"+str(median_delay_one_forbidden_api))\r\n",
    "        avg_delay_one_forbidden_glove, median_delay_one_forbidden_glove = get_avg_prediction_delay(prediction_one_forbidden_glove, labels_one_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_one_forbidden_glove)+\" median = \"+str(median_delay_one_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay two forbidden words:\")\r\n",
    "        avg_delay_two_forbidden_api, median_delay_two_forbidden_api = get_avg_prediction_delay(prediction_two_forbidden_api, labels_two_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_two_forbidden_api)+\" median = \"+str(median_delay_two_forbidden_api))\r\n",
    "        avg_delay_two_forbidden_glove, median_delay_two_forbidden_glove = get_avg_prediction_delay(prediction_two_forbidden_glove, labels_two_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_two_forbidden_glove)+\" median = \"+str(median_delay_two_forbidden_glove))\r\n",
    "        print()\r\n",
    "\r\n",
    "        print(\"Result avg delay three forbidden words:\")\r\n",
    "        avg_delay_three_forbidden_api, median_delay_three_forbidden_api = get_avg_prediction_delay(prediction_three_forbidden_api, labels_three_forbidden_api)\r\n",
    "        print(\"api: avg = \"+str(avg_delay_three_forbidden_api)+\" median = \"+str(median_delay_three_forbidden_api))\r\n",
    "        avg_delay_three_forbidden_glove, median_delay_three_forbidden_glove = get_avg_prediction_delay(prediction_three_forbidden_glove, labels_three_forbidden_glove)\r\n",
    "        print(\"glove: avg = \"+str(avg_delay_three_forbidden_glove)+\" median = \"+str(median_delay_three_forbidden_glove))\r\n",
    "        print()\r\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "perpetual_test_handle_forbidden_words()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}